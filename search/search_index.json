{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to TrinityLake","text":""},{"location":"#introduction","title":"Introduction","text":"<p>TrinityLake is an Open Lakehouse Format for Big Data Analytics, ML &amp; AI.  It defines different objects in a Lakehouse and provides a consistent and efficient way for accessing and manipulating these objects. The TrinityLake format offers the following key features:</p>"},{"location":"#multi-object-multi-statement-transactions","title":"Multi-Object Multi-Statement Transactions","text":"<p>TrinityLake enables multi-object multi-statement transactions across different tables, indexes, views,  materialized views, etc. within a Lakehouse. Users could start to leverage standard SQL BEGIN and COMMIT semantics and expect ACID enforcement  at SNAPSHOT or SERIALIZABLE isolation level across the entire Lakehouse.</p>"},{"location":"#consistent-time-travel-rollback-and-snapshot-export","title":"Consistent Time Travel, Rollback and Snapshot Export","text":"<p>TrinityLake provides a single timeline for all the transactions that have taken place within a Lakehouse. Users can perform time travel to get a consistent view of all the objects in the Lakehouse, rollback the Lakehouse to a consistent previous state, or choose to export a snapshot of the entire Lakehouse at any given point of time.</p>"},{"location":"#storage-only","title":"Storage Only","text":"<p>TrinityLake mainly leverages one storage primitive - mutual exclusion of file creation. This means you can run TrinityLake on almost any storage solution including Linux file system,  open source storage solutions like Apache Hadoop Distributed File System (HDFS) or Apache Ozone,  and cloud storage providers like Amazon S3, Google Cloud Storage, Azure Data Lake Storage.  You can build a truly open Lakehouse with TrinityLake without the need to pick a  Catalog / DataLake / Lakehouse / Warehouse vendor and worry about potential vendor lock-in risks.</p>"},{"location":"#compatibility","title":"Compatibility","text":""},{"location":"#open-table-formats","title":"Open Table Formats","text":"<p>TrinityLake can work with popular open table formats such as Apache Iceberg. Users can create and use these tables with both the traditional SQL <code>MANAGED</code> or <code>EXTERNAL</code> experience, as well as through federation when the table resides in other systems that can be connected to for read and write.</p>"},{"location":"#open-catalog-standards","title":"Open Catalog Standards","text":"<p>TrinityLake can be used as an implementation of open catalog standards like the Apache Iceberg REST Catalog (IRC) specification. The project provides an IRC server that users can run as a proxy to access TrinityLake and leverage all open source and  vendor products that support IRC. This provides a highly scalable yet extremely lightweight IRC implementation  where the IRC server is mainly just an AuthN &amp; AuthZ engine, and the main execution logic is pushed down to the storage layer and handled by this open Lakehouse format.</p>"},{"location":"#open-engines","title":"Open Engines","text":"<p>Through open table formats and open catalog standards, you can use TrinityLake with any open engine that supports them. In addition, TrinityLake is developing native connectors to various open engines such as Apache Spark. These native connectors will provide the full TrinityLake experience to users.</p>"},{"location":"#sidenote-why-the-name-trinitylake","title":"Sidenote: why the name TrinityLake?","text":"<p>Trinity Lake, previously called Clair Engle Lake, is a beautiful reservoir on the Trinity River formed by  the Trinity Dam and located in Trinity County, California, USA. Since we are building a format for Lakehouse, we decided to pick a name of a lake we like.</p> <p>We also like the term trinity as we see it symbolizes the 2 visions of this project:</p> <ol> <li>make catalog, table format, file format operate as one unified format on storage.</li> <li>run analytics, ML &amp; AI workloads all on the same Lakehouse platform.</li> </ol>"},{"location":"join-us/","title":"Join Us","text":"<p>This project is still at early development stage. If you are interested in developing this project with us together, we mainly use Slack (click for invite link) for communication. We also use GitHub Issues and GitHub Discussions for discussion purpose. </p>"},{"location":"format/key-encoding/","title":"Key Encoding","text":"<p>Note</p> <p>we use the literal <code>[space]</code> to represent the space character (hex value 20) in this document for clarity</p>"},{"location":"format/key-encoding/#system-internal-keys","title":"System Internal Keys","text":"<p>In general, system internal keys do not participate in the TrinityLake tree key sorting algorithm and always stay in  the designated node.</p>"},{"location":"format/key-encoding/#lakehouse-definition-key","title":"Lakehouse Definition Key","text":"<p>The Lakehouse definition file pointer is stored with key <code>lakehouse_def</code> in the root node.</p>"},{"location":"format/key-encoding/#previous-root-node-key","title":"Previous Root Node Key","text":"<p>The pointer to the previous root node is stored with key <code>previous_root</code> in the root node.</p>"},{"location":"format/key-encoding/#rollback-root-node-key","title":"Rollback Root Node Key","text":"<p>The pointer to the root node that was rolled back from, if the root node is created during a Rollback It is stored with key <code>rollback_from_root</code> in the root node.</p>"},{"location":"format/key-encoding/#creation-timestamp-key","title":"Creation Timestamp Key","text":"<p>The key <code>created_at_millis</code> writes the timestamp that a node is created.</p>"},{"location":"format/key-encoding/#number-of-keys-key","title":"Number of Keys Key","text":"<p>The key <code>n_keys</code> writes the number of keys that a node is currently having. This is used to determine the size of the node key table.</p>"},{"location":"format/key-encoding/#object-key","title":"Object Key","text":"<p>The object key is a UTF-8 string that uniquely identifies the object and also allows sorting it in a  lexicographical order that resembles the object hierarchy in a Lakehouse.</p>"},{"location":"format/key-encoding/#object-name","title":"Object Name","text":"<p>The object name has maximum size in bytes defined in Lakehouse definition file,  with one configuration for each type of object.</p> <p>The following UTF-8 characters are not permitted in an object name:</p> <ul> <li>any control characters (hex value 00 to 1F)</li> <li>the space character (hex value 20)</li> <li>the DEL character (hex value 7F)</li> </ul>"},{"location":"format/key-encoding/#encoded-object-name","title":"Encoded Object Name","text":"<p>When used in an object key, the object name is right-padded with space up to the maximum size  (excluding the initial byte). The maximum size of each object is defined in the Lakehouse definition file.</p> <p>For example, a namespace <code>default</code> under Lakehouse definition  <code>namespace_name_max_size_bytes=8</code> will have an encoded object name<code>[space]default[space]</code>.</p>"},{"location":"format/key-encoding/#encoded-object-definition-schema-id","title":"Encoded Object Definition Schema ID","text":"<p>The schema of the object definition has a numeric ID starting from 0,  and is encoded to a 4 character base64 string that uses the following encoding:</p> <ul> <li>Uppercase letters: A\u2013Z, with indices 0\u201325</li> <li>Lowercase letters: a\u2013z, with indices 26\u201351</li> <li>Digits: 0\u20139, with indices 52\u201361</li> <li>Special symbols: <code>+</code> and <code>/</code>, with indices 62\u201363</li> <li>Padding character <code>=</code>, which may only appear at the end of the string</li> </ul> <p>For example, schema ID <code>4</code> is encoded to <code>D===</code>.</p>"},{"location":"format/key-encoding/#object-key-format","title":"Object Key Format","text":"<p>The object key format combines the Encoded Object Name,  Encoded Object Definition Schema ID rules above to form a unique key  for each type of object. See the table below for the format for each type of object: (contents in <code>&lt;&gt;</code> should be substituted)</p> Object Type Schema ID Object ID Format Example Lakehouse 0 N/A, use Lakehouse Definition Key Namespace 1 <code>B===&lt;encoded namespace name&gt;</code> <code>B===default[space]</code> Table 2 <code>C===&lt;encoded namespace name&gt;&lt;encoded table name&gt;</code> <code>C===default[space]table[space][space][space]</code>"},{"location":"format/lakehouse-transaction/","title":"Lakehouse Transaction and ACID Enforcement","text":"<p>In this document, we describe how ACID properties are enforced at Lakehouse level, leveraging the Storage Transaction guarantees.</p>"},{"location":"format/lakehouse-transaction/#supported-isolation-levels","title":"Supported Isolation Levels","text":"<p>TrinityLake supports 2 isolation levels:</p> <ul> <li>SNAPSHOT ISOLATION</li> <li>SERIALIZABLE</li> </ul> <p>This means out of the box, TrinityLake users do not need to worry about potential dirty read, non-repeatable read and phantom read that could exist at lower isolation levels.</p> <p>Depending on tolerance of serialization anomaly like write skew, users can choose to set the isolation level to SNAPSHOT ISOLATION or SERIALIZABLE. Using SERIALIZABLE would ensure fully serial transaction history,  but reducing the transaction throughput of the whole Trinity lakehouse.</p>"},{"location":"format/lakehouse-transaction/#commit-conflict-resolution-strategy","title":"Commit Conflict Resolution Strategy","text":"<p>When there is a concurrent transaction commit failure, the failing transaction that started at version <code>v</code> should do the following:</p> <ol> <li>get the current latest version of the TrinityLake tree, denote this latest version as <code>w</code></li> <li>for each version between <code>v+1</code> and <code>w</code>, see if the changes can be applied while guaranteeing the isolation level of the transaction.<ol> <li>if all the changes can be applied up to version <code>w</code>, redo the storage commit process</li> <li>if not, the transaction has failed at the lakehouse level.</li> </ol> </li> </ol>"},{"location":"format/lakehouse-transaction/#example-concurrent-commits-to-2-different-tables","title":"Example: concurrent commits to 2 different tables","text":"<p>Consider 2 transactions, T1 updating table <code>t1</code> and T2 updating table <code>t2</code>, both started at lakehouse version 3. Assuming T1 commits successfully first, the following would happen for T2:</p> <ol> <li>T2 commit against version 4 fails, due to version 4 root node file already exists</li> <li>T2 checks the current latest version, which is 4</li> <li>T2 sees in root node file of version 4 that the last transaction made change to <code>t1</code></li> <li>T2 determines that its change in <code>t2</code> does not conflict with <code>t1</code></li> <li>T2 now applies its change against version 4 root node file</li> <li>T2 commits the new root node file against version 5</li> <li>T2 commit succeeds</li> </ol> <p>Note</p> <p>This conflcit resolution strategy is currently not fully correct.  For example, it is possible for update to <code>t2</code> to leverage infomration in <code>t1</code>,  causing the commit to be not serializable. We plan to introduce the concept of \"Object Change Definition\" to fix it, but in future iterations of the format. </p>"},{"location":"format/overview/","title":"Overview","text":"<p>Note</p> <p>The TrinityLake format specification requires specific knowledge about certain data structures, algorithms,  database system and file system concepts. If you find anything described difficult to understand, please follow  the corresponding links for further explanations, or make a contribution to help us improve this document.</p>"},{"location":"format/overview/#version","title":"Version","text":"<p>This document describes the TrinityLake format at version 0.0.0.  Please see Versioning about the versioning semantics of this format.</p>"},{"location":"format/overview/#introduction","title":"Introduction","text":"<p>The TrinityLake format defines a storage-only lakehouse  implemented using a modified version of the B-epsilon tree-based key-value map.</p> <ul> <li>The keys of this map are IDs of objects in a lakehouse</li> <li>The values of this map are location pointers to the Object Definitions </li> </ul> <p>We call such tree as the TrinityLake Tree,  and call a lakehouse implemented using the TrinityLake format as a Trinity Lakehouse.</p> <p>The TrinityLake format contains the following specifications:</p> <ul> <li>The TrinityLake tree is persisted in storage following Storage Layout Specification.</li> <li>The files in a TrinityLake tree are stored according to the Storage Location Specification.</li> <li>The TrinityLake tree is assessed and updated following the Transaction Specification.</li> <li>The keys in a TrinityLake tree follow the Key Encoding Specification.</li> <li>The object definitions in a TrinityLake tree follow the Object Definition File Specification.</li> </ul>"},{"location":"format/overview/#example","title":"Example","text":"<p>Here is an example logical representation of a TrinityLake tree:</p> <p></p> <p>This Trinity Lakehouse is a tree of order 3, with the following objects:</p> <ul> <li>Namespace <code>ns1</code><ul> <li>Table <code>table1</code></li> </ul> </li> <li>Namespace <code>ns2</code><ul> <li>Table <code>table1</code>: this is an Apache Iceberg table, which further points to its own metadata JSON file,   manifests Avro files and Parquet/Avro/ORC data files, following the Iceberg table format specification.</li> <li>Index <code>index1</code></li> </ul> </li> <li>Namespace <code>ns3</code><ul> <li>Materialized View <code>mv1</code></li> <li>Table <code>table1</code></li> </ul> </li> </ul> <p>There are also a set of write operations that are performed against the objects in the write buffer of the TrinityLake tree:</p> <ul> <li>Update the definition of existing table <code>table1</code> in <code>ns1</code></li> <li>Create a new materialized view <code>mv2</code> in namespace <code>ns2</code></li> </ul> <p>The root tree node is at version 4, and also points to the previous version of the root node of the Lakehouse. This is used for achieving time travel, rollback and snapshot export.</p>"},{"location":"format/storage-layout/","title":"Storage Layout","text":"<p>The TrinityLake tree in general follows the storage layout of N-way search tree map. In this document, we describe the details of the tree's layout in storage.</p>"},{"location":"format/storage-layout/#node-file-format","title":"Node File Format","text":"<p>Similar to a N-way search tree map,  each node of the TrinityLake tree is a node file in storage.  Each file fully describes tabular data using the Apache Arrow IPC format.</p>"},{"location":"format/storage-layout/#node-file-schema","title":"Node File Schema","text":"<p>The node file has the following schema:</p> ID Name Arrow Type Description Required? Default 1 key String Name of the key no 2 value String The value of the key no 3 pnode String Pointer to the path to the child node no 4 txn String Transaction ID for write buffer no"},{"location":"format/storage-layout/#node-file-content","title":"Node File Content","text":"<p>Each node file contains 3 sections from top to bottom:</p> <ul> <li>System internal rows</li> <li>Node key table</li> <li>Write buffer</li> </ul> <p>They all share the same node file schema above, but use it in different ways.</p>"},{"location":"format/storage-layout/#system-internal-rows","title":"System-Internal Rows","text":"<p>Only the <code>key</code> and <code>value</code> columns in the node file schema are meaningful to system internal rows, and they are required to be non-null.</p> <p>These rows are used for recording system internal information such as node creation time, version, etc. See system-internal keys for more details.</p> <p>There is no specific ordering expected for the system-internal rows, and there might be more system internal rows added over time. because the first row of the node key table must have <code>NULL</code> key and <code>NULL</code> value, readers of a node file are expected to treat all rows before this row as system internal rows.</p>"},{"location":"format/storage-layout/#node-key-table","title":"Node Key Table","text":"<p>To read the node key table, the reader should skip all system internal rows,  which means to skip all rows until it reaches the first row that has a <code>NULL</code> key and <code>NULL</code> value.</p> <p>Then based on the rules of the node key table, There are exactly <code>N</code> rows for the node key table section of the node file.</p>"},{"location":"format/storage-layout/#write-buffer","title":"Write Buffer","text":"<p>The B-epsilon tree-like write buffer of the TrinityLake tree starts after the system internal rows and the node key table rows.</p> <p>Each row in the write buffer represents a message to be applied to the TrinityLake tree. New messages are appended at the bottom of the write buffer. These rows have the following requirements:</p> <ol> <li><code>key</code> must not be <code>NULL</code></li> <li><code>transaction</code> must not be <code>NULL</code></li> <li><code>pnode</code> must be <code>NULL</code></li> <li>If <code>value</code> is <code>NULL</code>, it is a message to delete the key. If <code>value</code> is not <code>NULL</code>, it is a message to set the key to the specific value.</li> </ol> <p>Note that different from a standard B-epsilon tree, when flushing write buffer against the TrinityLake tree during a write or  compaction, the messages in the latest committed transaction will not be flushed, because it will be used for ensuring different level of isolation guarantee during Trinity Lakehouse commit phase. See Transaction and ACID Enforcement for more details.</p>"},{"location":"format/storage-layout/#node-file-size","title":"Node File Size","text":"<p>Each node is targeted for the same specific size, which is configurable in the Lakehouse definition. Based on those configurations, users can roughly estimate the size of the node key table as:</p> <pre><code>N * (\n  namespace_name_size_max_bytes + \n  table_name_size_max_bytes + \n  file_path_size_max_bytes +\n  4 bytes for schema ID with padding \n)\n</code></pre> <p>and this value must be less than the node file size. This remaining size is used as the write buffer for each node.</p> <p>For users that would like to fine-tune the performance characteristics of a TrinityLake tree, this formula can be used to readjust the node file size to achieve the desired epsilon value.</p>"},{"location":"format/storage-location/","title":"Storage Location","text":"<p>In the Storage Layout document, we have described how a TrinityLake tree is persisted in a storage. This document describes the specification for the location of persisted files.</p>"},{"location":"format/storage-location/#terminologies","title":"Terminologies","text":""},{"location":"format/storage-location/#directory-vs-prefix-notation","title":"Directory vs Prefix Notation","text":"<p>In general, TrinityLake is designed against object storage systems, and only have a prefix concept. There is no operation at TrinityLake interface level to create or manage directories in any storage systems that supports directory.</p> <p>However, we retain the conventional directory concept and syntax, and users can say a prefix, or a part of a prefix,  is a directory if the purpose of that prefix is to store more files, with <code>/</code> being the separator for different levels of directories.</p>"},{"location":"format/storage-location/#types-of-locations","title":"Types of Locations","text":"<p>We in general use 3 terminologies when describing the storage locations in TrinityLake:</p> <ul> <li>URI: a URI in TrinityLake follows the RFC-3986 specification,   but in addition obeys the object storage semantics, where special characters like <code>.</code>, <code>..</code>, etc. should be interpreted literally,   rather than having semantic file system meanings. To check if a URI is qualified to be used in TrinityLake, evaluate the URI in a   POSIX file system, and the resulting URI should be identical to the original URI. For example, <code>s3://my-bucket/../file</code> is evaluated   to <code>s3://my-file</code>, and is thus not qualified and must not be used in TrinityLake.</li> <li>Path: when used in TrinityLake, a path refers to a sequence of directories, optionally with a final file name.    Therefore, a path must be relative to a prefix, and not start with <code>/</code>.</li> <li>Location: when used in TrinityLake, a location can be either a path, or a fully qualified URI</li> </ul>"},{"location":"format/storage-location/#root-uri","title":"Root URI","text":"<p>A Trinity Lakehouse always starts at a storage Root URI, for example <code>s3://my-trinity-lakehouse/</code>. We call a storage system with a root URI a Trinity Lakehouse Storage, as all the internal information of this lakehouse must be using paths within this root URI.</p> <p>The root URI is always a directory, thus to avoid user confusion, we will always treat it as  ending with a <code>/</code> even when the user input does not. For example, if the user defines the root URI as <code>s3://my-bucket/my-lakehouse</code>, it is treated as <code>s3://my-bucket/my-lakehouse/</code> when used.</p> <p>The Lakehouse root URI is not stored within the TrinityLake format itself. It is expected that a user would specify the root location at runtime. This ensures the whole lakehouse is portable for use cases like cross-region replication.</p>"},{"location":"format/storage-location/#standard-file-paths","title":"Standard File Paths","text":""},{"location":"format/storage-location/#file-path-optimization","title":"File Path Optimization","text":"<p>A file path in the TrinityLake format is designed for optimized performance in storage, with a focus on object storage. Given an Original File Path, the Optimized File Path in storage can be calculated as the following:</p> <ol> <li>Calculate the MurMur3 hash of the file path in bytes.</li> <li>Get the first 20 bytes and convert it to binary string representation and use it as the prefix.     This maximizes the throughput in object storages like S3.</li> <li>For the first, second and third group of 4 characters in the prefix, further separated with <code>/</code>.     This maximizes the throughput in file systems when a directory listing at root location is necessary.</li> <li>Replace the <code>/</code> character in original file path by <code>-</code></li> <li>Concatenate the prefix before the file path produced in step 4 using the <code>-</code> character.</li> </ol> <p>For example, an original file path <code>my/path/my-table-definition.binpb</code> will be transformed to  <code>0101/0101/0101/10101100-my-path-my-table-definition.binpb</code>.</p> <p>Warning</p> <p>File name optimization is a write side feature, and should not be used by readers to reverse-engineer the original file name.</p>"},{"location":"format/storage-location/#non-root-node-file-path","title":"Non-Root Node File Path","text":"<p>Non-root node file paths are in the form of prefix <code>node-</code> plus a version 4 UUID with suffix <code>.ipc</code>. For example, if a UUID <code>6fcb514b-b878-4c9d-95b7-8dc3a7ce6fd8</code> is generated for the node file, the original file name of the node file will be <code>node-6fcb514b-b878-4c9d-95b7-8dc3a7ce6fd8.ipc</code>, and that further goes through the file name optimization to produce the final node file path.</p>"},{"location":"format/storage-location/#object-definition-file-path","title":"Object Definition File Path","text":"<p>Object definition file paths excluding the Lakehouse definition file path are in the form of prefix <code>{object-type}-{object-identifier}-</code> plus a version 4 UUID with suffix <code>.binpb</code>. For example, a table <code>t1</code> in namespace <code>ns1</code> and UUID <code>6fcb514b-b878-4c9d-95b7-8dc3a7ce6fd8</code> would give a path <code>table-t1-ns1-6fcb514b-b878-4c9d-95b7-8dc3a7ce6fd8.binpb</code>, and that further goes through the file name optimization to produce the final object definition file path.</p>"},{"location":"format/storage-location/#non-standard-file-paths","title":"Non-Standard File Paths","text":""},{"location":"format/storage-location/#root-node-file-path","title":"Root Node File Path","text":"<p>With CoW, the root node file name is important because every change to the tree would create a new root node file, and the root node file name can be used essentially as the version of the tree.</p> <p>TrinityLake defines that each root node has a numeric version number, and the root node is stored in a file name <code>_&lt;version_number_binary_reversed&gt;.ipc</code>. The file name is persisted in storage as is without optimization. For example, the 100th version of the root node file would be stored with name <code>_00100110000000000000000000000000.ipc</code>.</p>"},{"location":"format/storage-location/#root-node-latest-version-hint-file-path","title":"Root Node Latest Version Hint File Path","text":"<p>A file with name <code>_latest_hint.txt</code> is stored and marks the hint to the latest version of the TrinityLake tree root node file. The file name is persisted in storage as is without optimization The file contains a number that marks the presumably latest version of the tree root node, such as <code>100</code>.</p>"},{"location":"format/storage-location/#lakehouse-definition-file-path","title":"Lakehouse Definition File Path","text":"<p>Lakehouse definition file path are in the form of <code>_lakehouse_def_</code> plus a version 4 UUID with suffix <code>.binpb</code>. For example, a UUID <code>6fcb514b-b878-4c9d-95b7-8dc3a7ce6fd8</code> would give a path <code>_lakehouse_def_6fcb514b-b878-4c9d-95b7-8dc3a7ce6fd8.binpb</code>.</p>"},{"location":"format/storage-transaction/","title":"Storage Transaction and ACID Enforcement","text":"<p>In this document, we discuss how the TrinityLake integrates with any storage system  to deliver transactional features.</p>"},{"location":"format/storage-transaction/#storage-requirements","title":"Storage Requirements","text":"<p>A storage used in TrinityLake must have the following properties:</p>"},{"location":"format/storage-transaction/#basic-operations","title":"Basic Operations","text":"<p>A storage must support the following operations:</p> <ul> <li>Read a file to a given location</li> <li>Write a file to a given location</li> <li>Delete a file at a given location</li> <li>Check if a file exists at a given location</li> <li>List files sharing the same prefix</li> </ul>"},{"location":"format/storage-transaction/#mutual-exclusion-of-file-creation","title":"Mutual Exclusion of File Creation","text":"<p>A storage supports mutual exclusion of file creation when only one writer wins if there are multiple writers trying to write to the same new file. This is the key feature that TrinityLake relies on for enforcing ACID semantics during the commit process.</p> <p>This feature is widely available in most storage systems, for examples:</p> <ul> <li>On Linux File System through O_EXCL</li> <li>On Hadoop Distributed File System through atomic rename</li> <li>On Amazon S3 through IF-NONE-MATCH</li> <li>On Amazon DynamoDB through conditional PutItem</li> <li>On Google Cloud Storage through IF-NONE-MATCH</li> <li>On Azure Data Lake Storage through IF-NONE-MATCH</li> </ul>"},{"location":"format/storage-transaction/#consistency","title":"Consistency","text":"<p>The Consistency aspect of ACID is enforced by the storage, and TrinityLake format requires a Strongly Consistent storage. This means all operations that modify the storage (e.g. write, delete) are committed reflected immediately in any  subsequent read operations (e.g. read, list).  For example, the TrinityLake format would not work as expected if you use it on eventually consistent systems like Apache Cassandra.</p>"},{"location":"format/storage-transaction/#durability","title":"Durability","text":"<p>The Durability aspect of ACID is enforced in the storage system and out of the control of the TrinityLake format. For example, if you use the TrinityLake format on Amazon S3, you get 99.999999999% (11 9's) durability guarantee.</p>"},{"location":"format/storage-transaction/#storage-commit-process","title":"Storage Commit Process","text":""},{"location":"format/storage-transaction/#copy-on-write","title":"Copy-on-Write","text":"<p>Modifying a TrinityLake tree in storage means modifying the content of the existing node files and creating new node files. This modification process is Copy-on-Write (CoW), because \"modifying the content\" entails reading the existing content of the node file, and rewriting a completely new node file that contains potentially parts of the existing content plus the updated content.</p>"},{"location":"format/storage-transaction/#file-immutability","title":"File Immutability","text":"<p>When performing CoW, the node files are required to be created at a new file location, rather than overwriting an existing file. This means all node files are immutable once written until deletion through garbage collection process.</p>"},{"location":"format/storage-transaction/#commit-atomicity","title":"Commit Atomicity","text":"<p>When committing a transaction, the writer does the following:</p> <ol> <li>Flush write buffers if necessary and write all impacted non-root node files</li> <li>Try to write to the root node file with new write buffer in the targeted root node file name<ol> <li>If this write is successful, the transaction is considered succeeded. Write the <code>_latest_hint</code> file with the new version with best effort.</li> <li>If this write is not successful, the transaction commit step has failed at the storage layer. Depending on the overall lakehouse transaction     level, it might be possible to rebase and retry the commit. Read Lakehouse Transaction for more details.     if rebased commit is not possible, the transaction is considered failed and not retryable.</li> </ol> </li> </ol>"},{"location":"format/storage-transaction/#storage-read-isolation","title":"Storage Read Isolation","text":"<p>Based on the commit process of TrinityLake, every change to a Trinity Lakehouse must produce a new version of the TrinityLake tree root. This is the core feature used for read isolation. A transaction, either for read or write or both, will always start with identifying the version of the TrinityLake tree  to look into. This version is determined by:</p> <ol> <li>Reading the version hint file if the file exists, or start from version 0.     This is because at step 3 of Commit Atomicity, the write of the <code>_latest_hint</code> file is not guaranteed to exist or be accurate.     For example, if two processes A and B commit sequentially at version 2 and 3, but A wrote the hint slower than B,     the hint file will be incorrect with value 2.</li> <li>Try to get files of increasing version number until the version <code>k</code> that receives a file not found error</li> <li>The version <code>k-1</code> will be the one to decide the root node file path</li> </ol> <p>All the initial object definition resolutions within the specific transaction must happen using that version of the TrinityLake tree. This ensures all the transactions begin with a specific version that is isolated from other concurrent processes. For how to commit transactions to a Trinity Lakehouse while ensuring a specific ANSI-compliant isolation level guarantee,  read Lakehouse Transaction for more details.</p>"},{"location":"format/storage-transaction/#time-travel","title":"Time Travel","text":"<p>Because the TriniyLake tree node is versioned, time travel against the tree root,  i.e. time travel against the entire Trinity Lakehouse, is possible.</p> <p>The engines should match the time travel ANSI-SQL semantics in the following way:</p>"},{"location":"format/storage-transaction/#for-system_time-as-of","title":"FOR SYSTEM_TIME AS OF","text":"<p>The timestamp-based time travel can be achieved by continuously tracing the previous root node key  to older root nodes, and check the creation timestamp key until the right root node for time travel is found.</p>"},{"location":"format/storage-transaction/#for-system_version-as-of","title":"FOR SYSTEM_VERSION AS OF","text":"<p>When the system version is a numeric value, it should map to the version of the tree root node. The root node of the specific version can directly be found based on the root node file name.</p> <p>When the system version is a string that does not resemble a numeric value, it should map to a possible exported snapshot.</p>"},{"location":"format/storage-transaction/#rollback-committed-version","title":"Rollback Committed Version","text":"<p>TrinityLake uses the roll forward technique for rolling back any committed version. If the current latest root node version is <code>v</code>, and a user would like to rollback to version <code>v-1</code>, Rollback is performed by committing a new root node with version <code>v+1</code> which is most identical to the root node file <code>v-1</code>, with the difference that the root node <code>v</code> should be recorded as the rollback root node key.</p>"},{"location":"format/storage-transaction/#snapshot-export","title":"Snapshot Export","text":"<p>A snapshot export for a Trinity Lakehouse means to export a specific version of the TrinityLake tree root node, and all the files that are reachable through that root node.</p> <p>Every time an export is created, the Lakehouse definition should be updated to record the name of the export and the root node file that the export is at.</p> <p>There are many types of export that can be achieved, because the export process can decide to stop replication at any level of the tree and call it an export. At one extreme, a process can replicate any reachable files starting at the root node. We call this a Full Export. On the other side, a process can simply replicate the specific version of tree root node,  and all other files reachable from the root node are not replicated. We call this a Minimal Export. We call any export that is in between a Partial Export.</p> <p>Any file that is referenced by both the exported snapshot and the source Lakehouse might be removed by the  Lakehouse version expiration process. With a full snapshot export, all files are replicated and dereferenced from the source Lakehouse. With a partial or minimal export, additional retention policy settings are required to make sure the version expiration process still keep those files available for a certain amount of time.</p>"},{"location":"format/versioning/","title":"Versioning","text":"<p>The TrinityLake format is released independently. This means the format version could differ from the format implementation version. For example, you can have TrinityLake format <code>1.2.0</code>, Java SDK <code>1.5.1</code>.</p>"},{"location":"format/versioning/#compatibility-definitions","title":"Compatibility Definitions","text":"<p>We in general expect there to be 2 actors, Readers and Writers. Typically, reader and writer versions are at the same version, but a system can also choose to implement a reader and writer that are of different versions.</p>"},{"location":"format/versioning/#backward-compatible","title":"Backward Compatible","text":"<p>A format version is backward compatible if the format that is produced by a lower version writer can be correctly read by a higher version reader.</p>"},{"location":"format/versioning/#forward-compatible","title":"Forward Compatible","text":"<p>A format version is forward compatible if the format that is produced by a higher version writer can be correctly read by a lower version reader.</p>"},{"location":"format/versioning/#versioning-semantics","title":"Versioning Semantics","text":"<p>The TrinityLake format uses traditional major, minor and patch versioning semantics, forming a version string like <code>1.2.3</code>. We expect to bump up:</p> <ul> <li>Major version when the format introduces forward incompatible changes.</li> <li>Minor version for any new feature release in the format that is still forward compatible.</li> <li>Patch version if there are bugs, typos, improvements in wording, additional explanations, etc. that are added to the format.</li> </ul>"},{"location":"format/versioning/#format-implementation-expectations","title":"Format Implementation Expectations","text":"<p>In general, a TrinityLake format implementation is expected to be always backward compatible for all past versions, until a past version is declared as deprecated.</p> <p>Because of the backward and forward compatibility requirement, minor and patch versions are for information only so that people can know what has been changing and update their implementations accordingly. This is also why only the major version is directly recorded in the Lakehouse definition.</p> <p>It is recommended that format implementations explicitly check the format major version and  fail the reader or writer accordingly for unsupported major version.</p>"},{"location":"format/definitions/lakehouse/","title":"Lakehouse","text":"<p>Lakehouse is the top level container.</p>"},{"location":"format/definitions/lakehouse/#object-definition-schema","title":"Object Definition Schema","text":"<p>Schema ID: 0</p> Field Name Protobuf Type Description Required? Default name string A user-friendly name of this Lakehouse Yes major_version uint32 The major version of the format No 0 order uint32 The order of the B-epsilon tree No 128 namespace_name_max_size_bytes uint32 The maximum size of a namespace name in bytes No 100 table_name_max_size_bytes uint32 The maximum size of a table name in bytes No 100 file_name_max_size_bytes uint32 The maximum size of a file name in bytes No 200 node_file_max_size_bytes uint64 The maximum size of a node file in bytes No 1048576 (1MB) properties map Free form user-defined key-value string properties No maximum_version_age_millis uint64 Maximum age of a version before expiration No 7 days minimum_versions_to_keep uint32 The minimum number of versions to keep No 3 maximum_version_age_millis_overrides map The mapping of versions to their maximum age before expiration, if different from <code>maximum_version_age_millis</code> No exported_snapshots map The mapping of snapshot export name and corresponding root node file location No <p>Note</p> <p>An update to some of the fields would entail a potentially expensive change of the TrinityLake tree. For example, changing the maximum object size or file size would entail re-encode all the keys in the tree.</p>"},{"location":"format/definitions/namespace/","title":"Namespace","text":"<p>A namespace is a container in a Lakehouse to organize different objects.</p>"},{"location":"format/definitions/namespace/#object-definition-schema","title":"Object Definition Schema","text":"<p>Schema ID: 1</p> Field Name Protobuf Type Description Required? Default name string A user-friendly name of this namespace Yes properties map Free form user-defined key-value string properties No"},{"location":"format/definitions/namespace/#name-size","title":"Name Size","text":"<p>All namespace names must obey the maximum size configuration defined in the Lakehouse definition file.</p>"},{"location":"format/definitions/overview/","title":"Overview","text":""},{"location":"format/definitions/overview/#schema","title":"Schema","text":"<p>Each type of object definition has a different schema, which is defined using protobuf. The schema should evolve in a way that is backward and froward compatible following the versioning semantics.</p> <p>Each schema has a schema ID, and is used as a part of the TrinityLake tree key encoding.</p> <p>The TrinityLake format currently provides the following object definitions with their corresponding schemas:</p> <ul> <li>Lakehouse</li> <li>Namespace</li> <li>Table</li> </ul>"},{"location":"format/definitions/overview/#file-format","title":"File Format","text":"<p>The exact definition of each object is serialized into protobuf streams binary files, suffixed with <code>.binpb</code>. These files are called Object Definition Files (ODF).</p>"},{"location":"format/definitions/table/format-iceberg/","title":"Apache Iceberg Table Format","text":"<p>Apache Iceberg is one of the supported formats of a TrinityLake table.</p>"},{"location":"format/definitions/table/format-iceberg/#managed-iceberg-table","title":"Managed Iceberg Table","text":"<p>TrinityLake managed Iceberg tables should be created without any format properties in the table definition. The TrinityLake format determines what works the best for managing an Iceberg table within a Trinity Lakehouse.</p> <p>For a managed table, TrinityLake maps the data type to Iceberg in the following way:</p> TrinityLake Type Iceberg Type boolean boolean int2 integer int4 integer int8 long decimal(p, s) decimal(p, s) float4 float float8 double char(n) string varchar(n) string date date time(p) time if p=6, else long timetz long timestamp(p) timestamp if p=6, timestamp_ns if p=9, else long timestamptz(p) timestamptz if p=6, timestamptz_ns if p=9, else long fixed(n) fixed(n) varbyte(n) binary struct struct map map list list <p>Currently only Apache Parquet file format is supported when using a Trinitylake managed Iceberg table.</p>"},{"location":"format/definitions/table/format-iceberg/#external-iceberg-table","title":"External Iceberg Table","text":"<p>To use an external Iceberg table in TrinityLake, you can configure the following format properties:</p> Property Description Required? Default metadata_location The location of the Iceberg metadata file Yes schema_on_read If the table is schema on read. If true, a schema must be provided as a part of the table definition No false <p>For an external table, TrinityLake surfaces the data type in Iceberg to TrinityLake in the following way:</p> Iceberg Type TrinityLake Type boolean boolean integer int4 long int8 decimal(p, s) decimal(p, s) float float4 double float8 string varchar date date time if p=6, else long time(6) timestamp timestamp(6) timestamptz timestamptz(6) timestamp_ns timestamp(9) timestamptz_ns timestamptz(9) fixed(n) fixed(n) binary varbyte struct struct map map list list"},{"location":"format/definitions/table/format-iceberg/#federated-iceberg-table","title":"Federated Iceberg Table","text":"<p>To use a federated Iceberg table in TrinityLake, you need to configure Iceberg  catalog properties inside the format properties. TrinityLake will use the catalog properties to initialize an Iceberg catalog to federate into the external system to perform read and write. The federated table's data types will be surfaced to TrinityLake in the same way as external tables.</p>"},{"location":"format/definitions/table/overview/","title":"Overview","text":"<p>A table is a collection of related data organized in tabular format; consisting of columns and rows.</p>"},{"location":"format/definitions/table/overview/#object-definition-schema","title":"Object Definition Schema","text":"<p>Schema ID: 2</p> Field Name Protobuf Type Description Required? Default name string A user-friendly name of this table Yes schema Schema Schema of the table, see Table Schema Yes distribution_keys repeated uint32 The list of IDs for columns that are used as the distribution key No sort_keys repeated uint32 The list of IDs for columns that are used as sort key No primary_key repeated uint32 The list of IDs for columns that are used as primary key No unique_columns repeated uint32 The list of IDs for columns that are not used as primary key but are unique No table_type string Table type, see Table Type No MANAGED table_format string The format of the table, which decides the usage of <code>format_properties</code>. Currently <code>ICEBERG</code> is the only option. Yes format_properties map Free form format-specific key-value string properties, e.g. Apache Iceberg No properties map Free form user-defined key-value string properties No"},{"location":"format/definitions/table/overview/#name-size","title":"Name Size","text":"<p>All table names must obey the maximum size configuration defined in the Lakehouse definition file.</p>"},{"location":"format/definitions/table/table-schema/","title":"Schema","text":""},{"location":"format/definitions/table/table-schema/#primitive-types","title":"Primitive Types","text":"<p>TrinityLake tables support the following primitive data types:</p> Type Description Aliases boolean True or false bool int2 Signed 2-byte integer smallint int4 Signed 4-byte integer int, integer int8 Signed 8-byte integer bigint, long decimal(p, s) Exact numeric of selectable precision (p) and scale (s) numeric(p, s) float4 Single precision floating-point number real float8 Double precision flaoting-point number float8, float, double char(n) Fixed length character string of size n character, nchar, bpchar varchar(n) variable length character string of optional maximum size n character varying, nvarchar, text date Calendar date of year, month, day time(p) time of a day with precision p time without time zone timetz(p) time of a day with specific time zone with precision p time with time zone timestamp(p) Date and time on a wall clock with precision p timestamp without time zone timestamptz(p) Date and time with a time zone and with precision p timestamp with time zone fixed(n) Fixed length binary value of size n binary(n) Variable length binary value of optional maximum size n varbinary, binary varying, varbyte"},{"location":"format/definitions/table/table-schema/#nested-types","title":"Nested Types","text":"<p>TrinityLake tables support the following nested data types:</p> Type Description Aliases struct A tuple of typed values row map A collection of key-value pairs with a key type and a value type list A collection of values with some element type array"},{"location":"format/definitions/table/table-type/","title":"Table Type","text":"<p>There are 3 table types in TrinityLake, MANAGED, EXTERNAL and FEDERATED.</p>"},{"location":"format/definitions/table/table-type/#managed","title":"MANAGED","text":"<p>A managed table is fully compliant with the transaction semantics defined by the TrinityLake format. It can participate in multi-object and multi-statement transactions with any other managed objects in the same Trinity Lakehouse. When dropping the table, the data is also deleted.</p> <p>TrinityLake provides the overall semantics of a managed table in areas like schema, streaming and upsert behaviors, etc. and the behavior can be implemented using various table and file formats such as Apache Iceberg with Apache Parquet.</p>"},{"location":"format/definitions/table/table-type/#external","title":"EXTERNAL","text":"<p>An external table is managed by an external system that a Trinity Lakehouse has no knowledge about. It has 4 key characteristics:</p> <ol> <li>External tables are read-only, thus do not participate in write transactions.</li> <li>The external table definition is static and requires either manual refresh or some sort of pull/push-based mechanism to trigger the refresh.</li> <li>Schema on read is possible for user to define a specific read schema that does not need to comply with the underlying data source schema.</li> <li>When dropping the table, merely the table definition is dropped, the source table in the external system remains untouched.</li> </ol>"},{"location":"format/definitions/table/table-type/#federated","title":"FEDERATED","text":"<p>A federated table is managed by an external system that a Trinity Lakehouse can connect to  and perform read or write or both through the federation connection.</p> <p>Compared to external table, federated table could support more operations such as:</p> <ul> <li>Writing to the table</li> <li>Altering or dropping the source table definition</li> <li>Always reading the latest table without the need for manual or push/pull-based refresh</li> </ul> <p>The key characteristics of federated table from ACID perspective is that, the latest version of the table for read is not determined at transaction start time, but at the time that the table is initially loaded. This is similar to the Read Latest isolation mode that we see in managed table streaming, but for federated table it is even worse because we cannot make assumptions about how \"latest\" the response really is, and we also do not know the implication of writing to such a \"latest\" table.</p> <p>Because of this behavior, compared to managed table, federated table lowers the guarantee of the transactional  semantics provided by the TrinityLake format. If a user performs multi-table transaction against a managed table  with a federated table, the isolation level would be lowered to READ UNCOMMITTED level in the worst case. For example, a federated table could be rolling back while a managed table reads its data in a  JOIN operation, causing a dirty read.</p> <p>In addition, behaviors for SQL operations like <code>DROP TABLE</code> might not be strictly defined. If the federated system does not follow the standard SQL semantics for deleting data for its managed table, there is no way for TrinityLake to enforce the strict SQL semantics.</p> <p>In summary, the FEDERATED table type provides more capabilities and stronger flexibility than EXTERNAL table type, by trading off the strong SQL transactional ACID guarantees provided by the MANAGED table type  and should be used with caution.</p>"},{"location":"format/transaction/ansi-definitions/","title":"ANSI Definitions","text":"<p>In this document, we define the anomaly phenomena and isolation levels that are  available based on the ANSI-SQL standard and used by the TrinityLake format.</p>"},{"location":"format/transaction/ansi-definitions/#dirty-read","title":"Dirty Read","text":"<p>A dirty read is defined as the following:</p> <ol> <li>Transaction T1 modifies a data item.</li> <li>Another transaction T2 then reads that data item before T1 performs a COMMIT or ROLLBACK.</li> <li>If T1 then performs a ROLLBACK, T2 has read a data item that was never committed and so never really existed.</li> </ol> <p>For example:</p> Transaction T1 Transaction T2 BEGIN; SELECT age FROM users WHERE id = 1; -- returns 20 BEGIN; UPDATE users SET age = 21 WHERE id = 1; SELECT age FROM users WHERE id = 1; -- returns 21 ROLLBACK;"},{"location":"format/transaction/ansi-definitions/#non-repeatable-read","title":"Non-Repeatable Read","text":"<p>Non-repeatable read, a.k.a. fuzzy read, is defined as the following:</p> <ol> <li>Transaction T1 reads a data item.</li> <li>Another transaction T2 then modifies or deletes that data item and commits.</li> <li>If T1 then attempts to reread the data item, it receives a modified value or discovers that the data item has been deleted.</li> </ol> <p>For example:</p> Transaction T1 Transaction T2 BEGIN; SELECT age FROM users WHERE id = 1; -- returns 20 BEGIN; UPDATE users SET age = 21 WHERE id = 1; COMMIT; SELECT age FROM users WHERE id = 1; -- returns 21"},{"location":"format/transaction/ansi-definitions/#phantom-read","title":"Phantom Read","text":"<p>Phantom read is defined as the following:</p> <ol> <li>Transaction T1 reads a set of data items satisfying some search condition.</li> <li>Transaction T2 then creates, modifies or deletes data items that satisfy T1\u2019s search condition and commits.</li> <li>If T1 then repeats its read with the same search condition, it gets a set of data items different from the first read.</li> </ol> <p>For example:</p> Transaction T1 Transaction T2 BEGIN; SELECT count(*) FROM users WHERE age &lt; 20; -- returns 2 BEGIN; INSERT INTO users (1001, 'Amy', 18) COMMIT; SELECT count(*) FROM users WHERE age &lt; 20; -- returns 3"},{"location":"format/transaction/ansi-definitions/#ansi-isolation-levels","title":"ANSI Isolation Levels","text":"<p>In ANSI standard, there are 4 isolation levels are defined based on the 3 read phenomena:</p> Isolation Level / Read Phenomenon Dirty Read Non-Repeatable Read Phantom Read READ UNCOMMITTED Possible Possible Possible READ COMMITTED Possible Possible REPEATABLE READ Possible SERIALIZABLE <p>It is pretty clear that the first 3 isolation levels directly map to the read phenomena:</p> <ul> <li>READ UNCOMMITTED level could see uncommitted data and can cause dirty read</li> <li>READ COMMITTED level does not see the dirty read phenomenon</li> <li>REPEATABLE READ level does not see the non-repeatable read phenomenon</li> </ul> <p>Note</p> <p>In some commercial solutions, the REPEATABLE READ level guarantees more than the ANSI definition.  PostgreSQL guarantees both no non-repeated and phantom read.  Oracle REPEATABLE READ is actually SERIALIZABLE. In TrinityLake, we will stick with the ANSI definition.</p> <p>Naturally, the next isolation level should be called something like \u201cno-phantom read\u201d, but it is not the case. A system with SERIALIZABLE isolation emulates serial transaction execution for all committed transactions, as if transactions had been executed one after another, serially, rather than concurrently.</p> <p>SERIALIZABLE describes the ideal state of a database system, where all concurrent transactions can act as if they are committed sequentially. The implementation of SERIALIZABLE typically involves getting locks for all related rows and also query ranges, which reduces the concurrency of query.</p>"},{"location":"format/transaction/snapshot-isolation/","title":"Snapshot Isolation","text":""},{"location":"format/transaction/snapshot-isolation/#definition","title":"Definition","text":"<p>SNAPSHOT ISOLATION is an isolation level that also avoids dirty read,  non-repeatable read and phantom read like SERIALIZABLE.</p> <p>SNAPSHOT ISOLATION allows the transactions occurring concurrently to see the same snapshot or copy of the system  as it was at the beginning of the transactions, thus allowing a second transaction to make changes to the data that  was to be read by another concurrent transaction. This other transaction would not observe the changes made by the  second transaction and would continue working on the previous snapshot of the system.</p> <p>However, unlike SERIALIZABLE, concurrent transactions might not be able to act as if they are committed sequentially. When this happens, it is called a Serialization Anomaly</p>"},{"location":"format/transaction/snapshot-isolation/#write-skew","title":"Write Skew","text":"<p>At this isolation level, a serialization anomaly called Write Skew could occur. Write skew is defined as the following:</p> <ol> <li>Suppose transaction T1 reads x and y, which are consistent with constraint C.</li> <li>Then transaction T2 reads x and y, writes x, and commits.</li> <li>Then T1 writes y.</li> <li>If there were a constraint between x and y, commit serialization might be violated.</li> </ol> <p>A famous example is the black and white marble update. Consider a table of marbles:</p> ID Color 1 Black 2 Black 3 White 4 White <p>and 2 transactions can happen in the following order:</p> Transaction T1 Transaction T2 BEGIN; UPDATE marbles set color = 'White' WHERE color = 'Black'; BEGIN; UPDATE marbles set color = 'Black' WHERE color = 'White'; COMMIT; COMMIT; <p>Under SERIALIZABLE, transaction T1 will either fail to commit,  or commit all marbles to be white to present a serial commit history of T1 after T2.</p> <p>However, under SNAPSHOT ISOLATION, both transactions would succeed without conflict,  resulting in half marbles white and half black.  This result cannot be produced by any serial execution order of T1 and T2,  which means it is a serialization anomaly.  But this does not trigger any read phenomenon including phantom read,  because the search condition of <code>where color = 'Black'</code> does not overlap with <code>where color = 'White'</code>.</p>"},{"location":"format/tree/b-epsilon-tree/","title":"B-Epsilon Tree","text":"<p>As we see from the process of updating a B-tree,  The key issue with a normal B-tree is that the write amplification could be huge. An update might change a large portion of nodes in the tree, which is not desirable for a system with a lot of writes.</p>"},{"location":"format/tree/b-epsilon-tree/#background","title":"Background","text":"<p>A B-epsilon tree is a variant of B-tree that is more optimized for writes. It was originally proposed in Lower Bounds for External Memory Dictionaries  in 2003 as a way to demonstrate an asymptotic performance tradeoff curve between B-trees and buffered repository trees. The concept is re-introduced for database applications in An Introduction to Bepsilon-trees and Write-Optimization in 2015. One application of the B-epsilon tree in the storage domain is BtrFS,  an implementation of the Linux file system using this data structure.</p>"},{"location":"format/tree/b-epsilon-tree/#write","title":"Write","text":"<p>On top of the B-tree structure, B-epsilon tree introduces a Write Buffer in each node of the tree. The write buffer holds Messages about the operations to be performed in the tree. When a write happens, instead of updating a huge portion of the tree nodes,  the writer simply writes a message in the message buffer.</p> <p>If the message buffer is full, it sends the message down the node until a node where the buffer is not full, and also applies the existing messages in the buffer to the nodes along the way if possible. This behavior is called Flushing the Write Buffer.</p>"},{"location":"format/tree/b-epsilon-tree/#read","title":"Read","text":"<p>When reading, the reader starts from the root node and go down just like in B-tree. However, in addition to walking the tree, it needs to apply any messages in the write buffers at runtime to derive the latest value of a given key. This is technically a Merge-on-Read for people that are familiar with that terminology. </p>"},{"location":"format/tree/b-epsilon-tree/#compaction","title":"Compaction","text":"<p>Because of the delayed write mechanism using write buffer, a compaction is possible against the tree, where the process can force flushing all the messages in the buffers to the corresponding keys to clear up the buffer space. The process is not necessary because eventually the writes would bring down all the write buffers  to the right nodes to be applied, but doing compaction wisely would improve the write performance further.</p>"},{"location":"format/tree/b-tree/","title":"B-Tree","text":"<p>An N-way search tree only enforces the general requirements for the number of  children per tree node. The tree could become imbalanced over time. A B-tree of order N is a self-balancing  N-way search tree that enforces a set of rules when updating the tree:</p> <ol> <li>All leaf nodes must appear at the same level</li> <li>The root node must have at least 2 children, unless it is also a leaf</li> <li>All nodes, except for the root node and leaves, must have at least <code>\u2308N/2\u2309</code> children</li> </ol>"},{"location":"format/tree/b-tree/#example","title":"Example","text":"<p>There are many tutorials online talking about B-Tree algorithms. We will not describe all details here, but just demonstrate an example of how a B-tree is built from the bottom up. Consider this B-tree of order 3 as the initial state:</p> <p></p> <p>Consider putting a new key <code>80</code> to the tree.</p> <p>We start with going down the tree and putting the value to the correct leaf.</p> <p></p> <p>Because the leaf does not satisfy the N-way search tree requirement,  we split the node and move the middle value to the parent node.</p> <p></p> <p>Now the parent node does not satisfy the N-way search tree requirement, so we split the node and move the middle value to its parent node, which makes it a root node.</p> <p></p> <p>Now the tree satisfies the B-tree definition again, so the operation is completed.</p>"},{"location":"format/tree/search-tree-map/","title":"Search Tree Map","text":"<p>A search tree can not only be used as the implementation of a set, but also a key-value Map. This can be easily achieved by storing the value together with its corresponding key.  Here is a visual example 4-way search tree map:</p> <p></p>"},{"location":"format/tree/search-tree-map/#node-key-table","title":"Node Key Table","text":"<p>For each node of the search tree map, there is an internal mapping of key to value and child node pointers. A Node Key Table provides a way to describe such information in a tabular fashion.</p> <p>Every key, value and node pointer tuple forms a row in this node key table. There are exactly <code>N</code> rows for each node in a <code>N</code>-way search tree. The construction of the table follows the rules below:</p> <ol> <li>The first row must have <code>NULL</code> key and <code>NULL</code> value, and the node pointer (if exists) points to the leftest child node.</li> <li>Subsequent rows must be filled with non-null key and value from left to right.      The node pointer at the row (if exists) points to the right child node of the key at the same row.</li> <li>If there are less than <code>N-1</code> keys available to form <code>N</code> rows, the remaining rows are filled with all <code>NULL</code> values for key,      value and node pointer.</li> </ol> <p>For example, the node map of node 1 in the example above would look like:</p> Key Value Node Pointer NULL NULL /address/to/node2 k1 v1 /address/to/node3 k2 v2 /address/to/node4 NULL NULL NULL"},{"location":"format/tree/search-tree-map/#search-tree-map-with-value-as-pointers","title":"Search Tree Map with Value as Pointers","text":"<p>For TrinityLake, it is common for the values in map to be pointers / addresses / locations  that points to a much larger payload in memory or on disk.</p> <p>Consider a key-value map, where the value for a key is a file, then a 4-way search tree map could look like the following:</p> <p></p> <p>The same node 1 would have the following node key table:</p> Key Value Node Pointer NULL NULL /address/to/node2 k1 /address/to/f1 /address/to/node3 k2 /address/to/f2 /address/to/node4 NULL NULL NULL"},{"location":"format/tree/search-tree-map/#node-file","title":"Node File","text":"<p>To persist the whole tree in storage, each node key table is stored as a separated file that we call a Node File. For example, using this mechanism, the 4-way search tree above could look like the following 4 node files in S3:</p> <p></p>"},{"location":"format/tree/search-tree/","title":"Search Tree","text":"<p>A search tree is a tree data structure used for locating specific Keys from within a collection of keys, and used as an implementation of a Set.</p> <p>A search tree consists of a collection of Nodes, and each node contains an ordered collection of keys. For example, consider a search tree with integer keys:</p> <p></p> <p>To search if the key <code>64</code> is contained within the set:</p> <ol> <li>Start from the top node <code>(1, 31)</code>, <code>64</code> is greater than <code>31</code> so go right</li> <li>In <code>(58, 101)</code>, <code>64</code> is greater than <code>58</code> but smaller than <code>101</code>, so go down the pointer between <code>58</code> and <code>101</code></li> <li>In <code>(60, 64, 77)</code>, we find exact value for <code>64</code></li> <li>Conclusion reached that key <code>64</code> is contained within the set</li> </ol>"},{"location":"format/tree/search-tree/#n-way-search-tree","title":"N-Way Search Tree","text":"<p>A N-way search tree is a search tree where:</p> <ol> <li>A node with <code>k</code> children must have <code>k-1</code> number of keys.</li> <li>Each node must have a maximum of <code>N</code> child nodes (i.e. each node must have a maximum of <code>N-1</code> keys)</li> </ol> <p>The example above thus satisfies the requirement of being a 4-way search tree.</p>"}]}